{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cd0a5d0-363a-4bf9-a74d-c32c6dbe5bc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56077b66-c25b-4123-b193-c803b5f8a762",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf5f0b81-1179-4bea-bb91-3aeb6f87d36e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Table name\n",
    "table_name = \"housing\"\n",
    "\n",
    "# Load data from the table\n",
    "df = spark.read.table(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba1940e5-041d-49ea-8b75-8c547b331091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert pyspark dataframe to pandas dataframe\n",
    "df = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239fd19b-172e-45de-b029-af491b811753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca322e3-dd59-4aa5-af54-7e24e82f9220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1) Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97008e3b-eb4a-4c07-b220-1551b4eb3fbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c66a1d98-24df-4e2c-8595-bf120a35e4a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf44743b-e0b0-49c7-bfbc-04ade986f0ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.fillna(df.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e57ca040-e144-4753-a6db-7f70218db2c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d776b2-5be2-4cf9-a1c0-915605c2f68d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5136c1-3162-4080-a8c7-23191f4b5419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['ocean_proximity'] = df['ocean_proximity'].fillna(df['ocean_proximity'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42236df7-667f-4824-8581-9663c70e4e5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2) Outlier removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db1bdd1-58ce-4d77-93aa-c6c0d8ba32df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Outlier removal** is a process of identifying and removing or modifying data points that are considered unusual or extreme compared to the majority of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12148e59-cb13-49fb-b8d5-b4b4515e94b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**There are several methods commonly used to remove outliers from a DataFrame. Here are a few of them:**\n",
    "\n",
    "**1) Z-Score Method:**\n",
    "- Calculate the z-score for each value in the DataFrame.\n",
    "- Remove rows where any column has a z-score greater than a predefined threshold (e.g., 3).\n",
    "- This method assumes that the data follows a normal distribution.\n",
    "\n",
    "**2) IQR (Interquartile Range) Method:**\n",
    "- Calculate the IQR for each column in the DataFrame.\n",
    "- Remove rows where any column value is below the first quartile minus a multiple of the IQR or above the third quartile plus a multiple of the IQR (e.g., 1.5 times the IQR).\n",
    "- This method is robust to non-normal distributions.\n",
    "\n",
    "**3) Tukey's Fences Method:**\n",
    "- Calculate the lower and upper fences based on the first and third quartiles and the IQR.\n",
    "- Remove rows where any column value is below the lower fence or above the upper fence (e.g., 1.5 times the IQR).\n",
    "- Similar to the IQR method, this approach is robust to non-normal distributions.\n",
    "\n",
    "**4) Standard Deviation Method:**\n",
    "- Calculate the mean and standard deviation for each column in the DataFrame.\n",
    "- Remove rows where any column value is above or below a certain number of standard deviations from the mean (e.g., 3 standard deviations).\n",
    "- This method assumes a normal distribution of the data.\n",
    "\n",
    "**5) Percentile Method:**\n",
    "- Calculate the lower and upper percentiles for each column in the DataFrame (e.g., 1st and 99th percentiles).\n",
    "- Remove rows where any column value is below the lower percentile or above the upper percentile.\n",
    "- This method is not distribution-specific and removes extreme values.\n",
    "\n",
    "\n",
    "It's important to note that the choice of method depends on the characteristics of your data and the specific requirements of your analysis. You may need to experiment with different methods or use a combination of approaches to effectively remove outliers from your DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "43c05d24-54fa-4b84-9f00-35006d4c17da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## IQR (Interquartile Range) Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dd1303d-f71b-4495-a7d0-99ba9436b68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cae72b5-ac5c-43f6-9eab-be00d7a22ef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the numerical columns\n",
    "numerical_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                     'total_bedrooms', 'population', 'households', 'median_income',\n",
    "                     'median_house_value']\n",
    "\n",
    "# Calculate the first quartile (Q1) and third quartile (Q3) for each numerical column\n",
    "Q1 = df[numerical_columns].quantile(0.25)\n",
    "Q3 = df[numerical_columns].quantile(0.75)\n",
    "\n",
    "# Calculate the interquartile range (IQR) for each numerical column\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define the lower and upper bounds for outlier detection\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Filter out the outliers from the DataFrame\n",
    "df_no_outliers = df[~((df[numerical_columns] < lower_bound) | (df[numerical_columns] > upper_bound)).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcf138ad-fc56-4624-b9e9-e570f408c066",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f19a1b2-98a6-43cf-9a8b-a62a11cada92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the initial row count\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Calculate the row count after outlier removal\n",
    "final_row_count = len(df_no_outliers)\n",
    "\n",
    "# Calculate the number of removed rows\n",
    "removed_rows = initial_row_count - final_row_count\n",
    "\n",
    "# Display the number of removed rows\n",
    "print(\"Number of removed rows:\", removed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3be3686-6d62-4e1d-ac50-eea2618e2d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Z-Score Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "061091da-87a7-4d7a-bb67-8696bf6c9225",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Define the numerical columns\n",
    "numerical_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                     'total_bedrooms', 'population', 'households', 'median_income',\n",
    "                     'median_house_value']\n",
    "\n",
    "# Calculate z-scores for numerical columns\n",
    "z_scores = stats.zscore(df[numerical_columns])\n",
    "\n",
    "# Define the threshold for outlier detection\n",
    "threshold = 3\n",
    "\n",
    "# Filter out the outliers from the DataFrame\n",
    "df_no_outliers = df[(z_scores < threshold).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e380a2d0-9cea-4596-9ac1-d4dce0acc274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b752eeb3-7f3d-4184-9eed-635b9d0b7234",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the initial row count\n",
    "initial_row_count = len(df)\n",
    "\n",
    "# Calculate the row count after outlier removal\n",
    "final_row_count = len(df_no_outliers)\n",
    "\n",
    "# Calculate the number of removed rows\n",
    "removed_rows = initial_row_count - final_row_count\n",
    "\n",
    "# Display the number of removed rows\n",
    "print(\"Number of removed rows:\", removed_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b17847f2-bf20-4601-8814-c4a0ce06fc44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3) Feature Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ae0657-cb80-4814-9476-29679139ac13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_no_outliers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "041c5170-ce5f-4da9-bb67-190bc7356004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41db2438-bc80-4a1b-bf17-8407891f0079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b2836f4-3544-4f50-9c0e-de4022cfe6d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15d80674-6758-4eb9-8e06-7c30c096c95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431a7df1-f864-4150-b079-9a9799f07f7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[\"housing_median_age_days\"] = df[\"housing_median_age\"] * 365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b55c305-380b-4444-81fb-0ae31c6d59d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b6cf4cc-b400-4bd5-89c3-db37cd6a6240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"housing_median_age_days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c414750b-6cde-4335-8cbf-160ff9342927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbb5df3f-5601-4fa7-8e50-3bc73b8e8bcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4) Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "702c1382-ff41-4dc9-a0a3-86ee64b143ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Feature scaling, also known as data normalization:** The process of transforming numerical features in a dataset to a common scale. It is a crucial step in data preprocessing and feature engineering, as it helps to bring the features to a similar range and magnitude. The goal of feature scaling is to ensure that no single feature dominates the learning process or introduces bias due to its larger values\n",
    "\n",
    "- **There are two common methods for feature scaling:**\n",
    "\n",
    "**1) Standardization (Z-score normalization):** In this method, each feature is transformed to have zero mean and unit variance. The formula for standardization is: x_scaled = (x - mean) / standard_deviation.\n",
    "Standardization ensures that the transformed feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "**2) Min-Max scaling:** In this method, each feature is scaled to a specific range, typically between 0 and 1.\n",
    "The formula for min-max scaling is: x_scaled = (x - min) / (max - min).\n",
    "Min-max scaling preserves the relative ordering of values and ensures that the transformed feature is bounded within the defined range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0734eed5-82ed-427d-98e0-f3191fb03f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Feature scaling is important for several reasons:**\n",
    "\n",
    "1) Gradient-based optimization algorithms, such as gradient descent, converge faster when features are on a similar scale. This helps in achieving faster convergence and more efficient training of machine learning models.\n",
    "\n",
    "2) Features with larger scales can dominate the learning process, leading to biased results. Scaling the features ensures that no single feature has undue influence on the model.\n",
    "\n",
    "3) Many machine learning algorithms, such as K-nearest neighbors (KNN) and support vector machines (SVM), rely on calculating distances between data points. If features are not on a similar scale, features with larger values can dominate the distance calculations, leading to suboptimal results.\n",
    "\n",
    "4) Some algorithms, such as principal component analysis (PCA), assume that the data is centered and on a similar scale. Feature scaling is necessary to meet these assumptions and obtain meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4ff0a0-8313-423b-9031-d220ea042aee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "300db6fa-8de4-4a11-8181-6c743ca1267b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "numerical_features = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "       'median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19ef36b9-226f-49c1-be60-a25502d6f0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c93009d-eff1-45a2-9da4-d774c30d091a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d08d4d9d-f9aa-4226-8691-cde835e3033f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2baed524-7a78-4ab9-af4e-acc3fe92b550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "001ee991-06a5-489e-89e6-1b2874eb3b45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5) One-hot-encoding (Feature Encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b8b22f29-1080-4b08-b298-3f9074c90bc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One-hot encoding, also known as feature encoding, is a technique used to convert categorical variables into a numerical representation that can be used by machine learning algorithms. It is a common preprocessing step in machine learning tasks that involve categorical features.\n",
    "\n",
    "Categorical variables are variables that represent qualitative or discrete characteristics or groups. Examples of categorical variables include \"color\" (red, green, blue), \"city\" (New York, London, Paris), or \"animal\" (cat, dog, bird)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02ac1c73-a2c1-4ac8-afb6-4a8ff771aa40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**The benefits of one-hot encoding include:**\n",
    "\n",
    "1) Compatibility with machine learning algorithms: Many machine learning algorithms require numerical input. By converting categorical variables into a numerical format, one-hot encoding enables the use of these variables in machine learning models.\n",
    "\n",
    "2) Preserving information: One-hot encoding preserves the information about the presence or absence of specific categories in the original data, which can be valuable for certain models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0520a103-1958-49a3-8eff-f44bc62e5ba6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select_dtypes(include=\"object\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb190db5-d763-4a48-8d6c-34e686516b35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['ocean_proximity'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1fc0f4-da7c-484a-877e-30afa4236937",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df['ocean_proximity'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d18a374-a634-4537-b59c-01084141c19e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4a93300-760a-4a9e-acf3-1f5bb38bc04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df0a34d-19d5-400a-92f7-0506f2d9ec7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(data=df, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0bf0680-36e2-4650-8121-5e1c19103c6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2e7f332-f8e6-4834-bc07-3111d25fe0e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3865476b-2555-4fa8-907a-77d7c42f2e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6) Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19dbec9c-ab65-46a0-8be0-d1f4a24401ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Feature selection is a crucial step in feature engineering, where the goal is to identify and select a subset of relevant features from the available set of features in a dataset. The aim is to improve model performance, reduce overfitting, enhance interpretability, and reduce computational complexity.\n",
    "\n",
    "- **Benefits of feature selection include:**\n",
    "\n",
    "1) Improved model performance: By selecting relevant features, feature selection can enhance model accuracy, reduce overfitting, and improve generalization on unseen data.\n",
    "\n",
    "2) Faster model training: Fewer features can lead to faster training times, especially when dealing with large datasets or complex models.\n",
    "\n",
    "3) Enhanced interpretability: Selecting a subset of meaningful features can improve the interpretability of the model, allowing for better understanding and insights.\n",
    "\n",
    "4) Reduced dimensionality: By eliminating irrelevant or redundant features, feature selection can reduce the dimensionality of the dataset, making it more manageable and reducing the risk of the curse of dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb5af28-26dd-4bf8-b83e-dec9f2ea1f4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "318490d7-8c0d-4759-9adb-0c112739985b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"median_house_value\", axis=1)\n",
    "y = df[\"median_house_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3265861-5d58-44fd-b575-8ef1fbc1fd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "586742b2-11b4-4ea6-a561-b65e0e69a5f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ced3336-eb3b-4d2b-84b2-effe314925b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c49f826-131c-4f4a-ab98-9529580bd16b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create the feature selection model (linear regression estimator and 5 features to select)\n",
    "estimator = LinearRegression()\n",
    "rfe = RFE(estimator, n_features_to_select=5)\n",
    "\n",
    "# Fit the feature selection model on the data\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get the selected features\n",
    "selected_features = X.columns[rfe.support_]\n",
    "print(selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "37accbcc-597f-4097-85f1-c05595f42056",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 7) Feature Transformation (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc93f163-afca-4f98-ae80-a9fe0c7f690b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The process of applying mathematical or statistical transformations to the existing features in a dataset to make them more suitable for a machine learning algorithm or to reveal underlying patterns in the data.\n",
    "- Feature transformation techniques aim to improve the quality and representativeness of the features, which can lead to better model performance and more meaningful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a21cf1c2-11d6-4f10-9f2a-70bd37ce1b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'feature1': [10, 20, 30, 40, 50],\n",
    "    'feature2': [0.1, 1, 10, 100, 1000],\n",
    "    'feature3': [100, 200, 300, 400, 500]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a4f17e5-a3fd-43de-900c-1f4899d3bb7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45b28d8-f5f2-4cdf-bf6d-78eaae7c34e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Logarithmic Transformation\n",
    "log_transformed_feature = np.log(data['feature1'])\n",
    "\n",
    "# Power Transformation\n",
    "power_transformed_feature = np.sqrt(data['feature2'])\n",
    "\n",
    "# Box-Cox Transformation\n",
    "boxcox_transformed_feature, _ = boxcox(data['feature3'])\n",
    "\n",
    "# Binning\n",
    "bin_edges = [0, 20, 40, 60]\n",
    "binned_feature = pd.cut(data['feature1'], bins=bin_edges, labels=False)\n",
    "\n",
    "# Polynomial Transformation\n",
    "polynomial_features = pd.DataFrame({\n",
    "    'feature1_squared': data['feature1'] ** 2,\n",
    "    'feature1_cubed': data['feature1'] ** 3\n",
    "})\n",
    "\n",
    "# Interaction Terms\n",
    "interaction_feature = data['feature1'] * data['feature2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f82b6c9-881f-42ac-83a9-c45308d80f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the transformed features\n",
    "print(\"Normalized data:\")\n",
    "print(normalized_data)\n",
    "\n",
    "print(\"\\nStandardized data:\")\n",
    "print(standardized_data)\n",
    "\n",
    "print(\"\\nLogarithmic transformed feature:\")\n",
    "print(log_transformed_feature)\n",
    "\n",
    "print(\"\\nPower transformed feature:\")\n",
    "print(power_transformed_feature)\n",
    "\n",
    "print(\"\\nBox-Cox transformed feature:\")\n",
    "print(boxcox_transformed_feature)\n",
    "\n",
    "print(\"\\nBinned feature:\")\n",
    "print(binned_feature)\n",
    "\n",
    "print(\"\\nPolynomial features:\")\n",
    "print(polynomial_features)\n",
    "\n",
    "print(\"\\nInteraction feature:\")\n",
    "print(interaction_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63363145-1228-4c64-94ac-b2f26951332f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 8) Dimensionality Reduction (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a703e903-4ae5-463e-b1d8-6b4e5cb4ff97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- The process of reducing the number of features or variables in a dataset while preserving the essential information\n",
    "- **Aims to overcome,**\n",
    "1) The curse of dimensionality\n",
    "2) Improve computational efficiency\n",
    "3) Eliminate noise or redundant features\n",
    "4) Potentially enhance the performance of ML models\n",
    "\n",
    "- High-dimensional data can lead to several challenges, such as increased computational complexity, overfitting, and difficulty in interpreting and visualizing the data\n",
    "- Dimensionality reduction techniques address these challenges by transforming or projecting the data into a lower-dimensional space, where the most relevant information is retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "481b4746-570d-403d-bcdb-7aacc45c24a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**There are two main approaches to dimensionality reduction:**\n",
    "\n",
    "**1) Feature Selection:** This approach involves selecting a subset of the original features based on certain criteria. It aims to identify the most informative and relevant features that contribute significantly to the target variable or capture the underlying patterns in the data. Feature selection methods can be filter-based (e.g., correlation, statistical tests) or wrapper-based (e.g., recursive feature elimination, forward/backward feature selection).\n",
    "\n",
    "**2) Feature Extraction:** This approach involves transforming the original features into a new set of lower-dimensional features. It aims to create a compressed representation of the data by combining or projecting the original features into a new feature space. Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are popular feature extraction techniques. Other methods include Non-negative Matrix Factorization (NMF), t-SNE, and Autoencoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77aab2f5-1397-45f5-9e73-f1a3430c8dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Benefits of dimensionality reduction:**\n",
    "\n",
    "1) Computational Efficiency: By reducing the number of features, the computational complexity of algorithms decreases, resulting in faster training and inference times.\n",
    "\n",
    "2) Overfitting Prevention: Dimensionality reduction helps to remove noisy or irrelevant features, reducing the risk of overfitting and improving the generalization capability of models.\n",
    "\n",
    "3) Improved Visualization: Lower-dimensional data can be visualized more easily, enabling better understanding and interpretation of the data.\n",
    "\n",
    "4) Enhanced Model Performance: By focusing on the most relevant features, dimensionality reduction can improve the performance of machine learning models by reducing noise, capturing important patterns, and avoiding the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4b62e16-52c9-461b-9395-5d43f8b29590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Principal Component Analysis (PCA)** is a widely used technique for dimensionality reduction and feature extraction in data analysis and machine learning. It aims to transform a high-dimensional dataset into a lower-dimensional space while preserving the most important patterns and variations in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd88b87-8a6f-4d7e-a79e-07c5f3ece6e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the PCA model\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to X\n",
    "pca.fit(X)\n",
    "\n",
    "# Transform X to the new feature space\n",
    "X_reduced = pca.transform(X)\n",
    "\n",
    "# Print the shape of X_reduced\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efc5839-f19b-453a-bac4-664fca050db5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Print the number of principal components\n",
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3eb7edb-297f-484b-9ebf-d9ef8efde7ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(X_reduced)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2_2_Feature engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  },
  "name": "DataExploration-7dedab5f031a751d6b9b02925c10b3b8"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
